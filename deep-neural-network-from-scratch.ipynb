{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kisarak/deep-neural-network-from-scratch?scriptVersionId=219612326\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import numpy as np\nimport h5py\nimport matplotlib.pyplot as plt\nfrom testCases_v4 import *\nimport copy\nnp.random.seed(1)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-27T07:06:28.791Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. initialize_parameters\n## (layer_dims [list]) -> parameters [dict]\n\nImplement initialization for an L-layer Neural Network. ","metadata":{}},{"cell_type":"code","source":"def initialize_parameters(layer_dims):\n\n    parameters = {}\n    L = len(layer_dims) # number of layers in the network\n\n    for l in range(1, L):\n        \n        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n        \n    return parameters","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T05:56:53.009904Z","iopub.execute_input":"2025-01-27T05:56:53.010299Z","iopub.status.idle":"2025-01-27T05:56:53.027351Z","shell.execute_reply.started":"2025-01-27T05:56:53.010262Z","shell.execute_reply":"2025-01-27T05:56:53.026297Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## 2. layer_forward\n## (A_prev, W, b, activation) -> A, cache(in_cache, z_cache)\n\nBuild a single layer pass of forward propagation.\n\nThe layer forward module (vectorized over all the examples) computes the following equations:\n\n$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$$\n\n$$A^{[l]} = g(Z^{[l]})$$","metadata":{}},{"cell_type":"markdown","source":"returning `cache` inside the function is generally preferred and sometimes necessary in backpropagation, even though you could technically store Z outside the function.\n\nFunctions like `relu` should ideally be self-contained. By returning cache, the function ensures it provides all the information needed for both the forward and backward passes.","metadata":{}},{"cell_type":"code","source":"def sigmoid(Z):\n    \n    A = 1/(1+np.exp(-Z))\n    cache = Z\n    return A, cache\n\ndef relu(Z):\n    \n    A = np.maximum(0,Z)\n    cache = Z \n    return A, cache","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-27T07:06:28.791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def layer_forward(A_prev, W, b, activation):\n\n    Z = np.dot(W, A_prev) + b\n    in_cache = (A_prev, W, b)\n    \n    if activation == \"sigmoid\":\n        A, Z_cache = sigmoid(Z)\n        \n    elif activation == \"relu\":\n        A, Z_cache = relu(Z)\n\n    cache = (in_cache, Z_cache)\n\n    return A, cache","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T05:36:30.029886Z","iopub.execute_input":"2025-01-27T05:36:30.030287Z","iopub.status.idle":"2025-01-27T05:36:30.046716Z","shell.execute_reply.started":"2025-01-27T05:36:30.030254Z","shell.execute_reply":"2025-01-27T05:36:30.045543Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. model_forward\n## X, parameters -> AL, caches(cache 1 to L)","metadata":{}},{"cell_type":"code","source":"def model_forward(X, parameters):\n\n    caches = []\n    A = X\n    L = len(parameters) // 2\n    \n    for l in range(1, L):\n        A_prev = A \n        W, b = parameters['W' + str(l)], parameters['b' + str(l)]\n        A, cache = layer_forward(A_prev, W, b, 'relu') # <-- Forward Prop\n        caches.append(cache)\n    \n    W, b = parameters['W' + str(L)], parameters['b' + str(L)]\n    AL, cache = layer_forward(A, W, b, 'sigmoid')  # <-- Forward Prop\n    caches.append(cache)\n          \n    return AL, caches","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. compute_cost\nCompute the cross-entropy cost $J$, using the following formula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\left[y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)\\right]$$\n","metadata":{}},{"cell_type":"code","source":"def compute_cost(AL, Y):\n\n    # Binary Cross Entropy\n    \n    m = Y.shape[1]\n    cost = -1/m * (np.dot(Y, np.log(AL.T)) + np.dot(1-Y, np.log(1-AL.T)))\n\n    return np.squeeze(cost)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T06:08:43.375615Z","iopub.execute_input":"2025-01-27T06:08:43.375952Z","iopub.status.idle":"2025-01-27T06:08:43.392586Z","shell.execute_reply.started":"2025-01-27T06:08:43.375927Z","shell.execute_reply":"2025-01-27T06:08:43.391496Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"## 5. layer_backward\n\nIf $g(\\;)$ is the activation function, \n`sigmoid_backward` and `relu_backward` compute $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$  \n\nFor layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$\n\nIf you have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$.\n\nThe three outputs $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ are computed using the input $dZ^{[l]}$.\n\n$$ dW^{[l]} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$\n$$ db^{[l]} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)} $$\n$$ dA^{[l-1]} = W^{[l] T} dZ^{[l]} $$\n\nNote: We use the variable d(variable) to denote the partial derivative of $\\mathcal L$ with respect to (variable)\n\n$$dX \\rightarrow \\mathcal L _X = \\frac{\\partial \\mathcal{L} }{\\partial X}$$","metadata":{}},{"cell_type":"code","source":"def relu_backward(dA, cache):\n    \n    Z = cache\n    dZ = np.array(dA, copy=True)\n    \n    # When z <= 0, you should set dz to 0 as well. \n    dZ[Z <= 0] = 0\n    \n    return dZ\n\ndef sigmoid_backward(dA, cache):\n    Z = cache\n    \n    s = 1/(1+np.exp(-Z))\n    dZ = dA * s * (1-s)\n    \n    return dZ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T06:05:08.866172Z","iopub.execute_input":"2025-01-27T06:05:08.866525Z","iopub.status.idle":"2025-01-27T06:05:08.88451Z","shell.execute_reply.started":"2025-01-27T06:05:08.866497Z","shell.execute_reply":"2025-01-27T06:05:08.883329Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def layer_backward(dA, cache, activation):\n    linear_cache, activation_cache = cache\n    \n    if activation == \"relu\":\n        dZ = relu_backward(dA, activation_cache)\n        \n    elif activation == \"sigmoid\":\n        dZ = sigmoid_backward(dA, activation_cache)\n    \n    A_prev, W, b = linear_cache\n    m = A_prev.shape[1]\n    \n    dW = (1 / m) * np.dot(dZ, A_prev.T)\n    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n    dA_prev = np.dot(W.T, dZ)\n    \n    return dA_prev, dW, db","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T06:18:55.318391Z","iopub.execute_input":"2025-01-27T06:18:55.318735Z","iopub.status.idle":"2025-01-27T06:18:55.335783Z","shell.execute_reply.started":"2025-01-27T06:18:55.318707Z","shell.execute_reply":"2025-01-27T06:18:55.334524Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def model_backward(AL, Y, caches):\n    \n    grads = {}\n    L = len(caches)\n    m = AL.shape[1]\n    Y = Y.reshape(AL.shape)\n    \n    dAL = -(np.divide(Y, AL) - np.divide(1-Y, 1-AL))\n    current_cache = caches[L-1]\n    dA_prev_temp, dW_temp, db_temp = layer_backward(dAL, current_cache, activation=\"sigmoid\")\n    grads[\"dA\" + str(L-1)] = dA_prev_temp\n    grads[\"dW\" + str(L)] = dW_temp\n    grads[\"db\" + str(L)] = db_temp\n    \n    for l in reversed(range(L-1)):\n        current_cache = caches[l]\n        dA_prev_temp, dW_temp, db_temp = layer_backward(grads[\"dA\" + str(l+1)], current_cache, activation=\"relu\")\n        grads[\"dA\" + str(l)] = dA_prev_temp\n        grads[\"dW\" + str(l+1)] = dW_temp\n        grads[\"db\" + str(l+1)] = db_temp\n\n    return grads","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T06:20:57.917788Z","iopub.execute_input":"2025-01-27T06:20:57.918167Z","iopub.status.idle":"2025-01-27T06:20:57.937709Z","shell.execute_reply.started":"2025-01-27T06:20:57.918137Z","shell.execute_reply":"2025-01-27T06:20:57.936304Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def update_parameters(params, grads, learning_rate):\n    \n    parameters = copy.deepcopy(params)\n    L = len(parameters) // 2 # number of layers in the neural network\n\n    for l in range(L):\n        \n        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n    \n    return parameters","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T06:21:54.508269Z","iopub.execute_input":"2025-01-27T06:21:54.508633Z","iopub.status.idle":"2025-01-27T06:21:54.525585Z","shell.execute_reply.started":"2025-01-27T06:21:54.508599Z","shell.execute_reply":"2025-01-27T06:21:54.524512Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"update_parameters(*update_parameters_test_case(), 0.1)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-27T07:06:28.791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}